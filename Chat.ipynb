{
 "cells": [
  {
   "metadata": {
    "executionRelatedData": {
     "compiledClasses": [
      "Line_2_jupyter",
      "Line_3_jupyter",
      "Line_4_jupyter",
      "Line_2_jupyter"
     ]
    },
    "ExecuteTime": {
     "end_time": "2025-10-16T15:25:56.575415Z",
     "start_time": "2025-10-16T15:25:56.050365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@file:DependsOn(\"ai.koog:koog-agents-jvm:0.5.0\")\n",
    "%use coroutines"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "collapsed": true,
    "executionRelatedData": {
     "compiledClasses": [
      "Line_5_jupyter"
     ]
    },
    "ExecuteTime": {
     "end_time": "2025-10-16T15:25:56.679980Z",
     "start_time": "2025-10-16T15:25:56.580835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ai.koog.prompt.executor.ollama.client.OllamaClient\n",
    "\n",
    "val ollama = OllamaClient()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T15:26:04.850735Z",
     "start_time": "2025-10-16T15:25:56.714863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ai.koog.prompt.dsl.prompt\n",
    "import ai.koog.prompt.executor.clients.openai.OpenAIModels\n",
    "import ai.koog.prompt.llm.LLMCapability\n",
    "import ai.koog.prompt.llm.LLMProvider\n",
    "import ai.koog.prompt.llm.LLModel\n",
    "import ai.koog.prompt.llm.OllamaModels\n",
    "import ai.koog.prompt.message.Message\n",
    "\n",
    "val GptOss = LLModel(\n",
    "    provider = LLMProvider.Ollama,\n",
    "    id = \"gpt-oss:20b\",\n",
    "    capabilities = listOf(\n",
    "        LLMCapability.Temperature,\n",
    "        LLMCapability.Schema.JSON.Standard,\n",
    "        LLMCapability.Tools\n",
    "    ),\n",
    "    contextLength = 128_000,\n",
    ")\n",
    "\n",
    "runBlocking {\n",
    "    val responses = ollama.execute(prompt(\"name\") {\n",
    "        system(\"Helpfull assistant!\")\n",
    "        user(\"Tell me a Kotlin Joke?\")\n",
    "    }, GptOss)\n",
    "\n",
    "    responses.forEach { response ->\n",
    "        when (response) {\n",
    "            is Message.Assistant -> println(response.content)\n",
    "            is Message.Tool.Call -> TODO()\n",
    "        }\n",
    "    }\n",
    "}"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s one that keeps the “null‑safe” vibes alive:\n",
      "\n",
      "> **Why did the Kotlin developer cross the road?**  \n",
      "> **To get to the null‑safe side!**\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T15:26:06.625450Z",
     "start_time": "2025-10-16T15:26:04.857551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ai.koog.prompt.xml.xml\n",
    "import ai.koog.prompt.dsl.PromptBuilder\n",
    "import ai.koog.prompt.message.Message\n",
    "\n",
    "fun PromptBuilder.userQuestion(question: String) {\n",
    "    user {\n",
    "        xml {\n",
    "            tag(\"user-question\") {\n",
    "                +question\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "val system = prompt(\"chat\") {\n",
    "    system {\n",
    "        xml {\n",
    "            tag(\"objective\") { text(\"Helpfull assistant!\") }\n",
    "            tag(\"instructions\") {\n",
    "                tag(\"critical\") {\n",
    "                    +\"Never hallucinate any data, and strictly answer to users questions.\"\n",
    "                    +\"If you cannot answer the question you must say so.\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "val chat1 = prompt(system) {\n",
    "    userQuestion(\"Tell me a Kotlin Joke.\")\n",
    "}\n",
    "\n",
    "val responses: List<Message.Response> = runBlocking {\n",
    "    ollama.execute(chat1, GptOss)\n",
    "}\n",
    "\n",
    "for(response in responses) {\n",
    "    when(response) {\n",
    "        is Message.Assistant -> TODO()\n",
    "        is Message.Tool.Call -> TODO()\n",
    "    }\n",
    "}\n",
    "\n",
    "responses"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Assistant(content=Here’s one that developers usually get a chuckle out of:\n",
       "\n",
       "> Why did the Kotlin developer break up with Java?\n",
       "\n",
       "> Because they couldn’t handle all the *nullable* arguments, and it turned out the relationship was just too *fun‑ctor* for them!, metaInfo=ResponseMetaInfo(timestamp=2025-10-16T15:26:06.602201Z, totalTokensCount=274, inputTokensCount=141, outputTokensCount=133, additionalInfo={}, metadata=null), attachments=[], finishReason=null)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T09:45:52.046225Z",
     "start_time": "2025-10-16T09:45:50.479634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ai.koog.prompt.text.TextContentBuilder\n",
    "\n",
    "val chat2 = prompt(chat1) {\n",
    "    messages(responses)\n",
    "    userQuestion(\"That was a bad joke! Tell me another better one.\")\n",
    "}\n",
    "\n",
    "val responses2 = runBlocking {\n",
    "    ollama.execute(chat2, GptOss)\n",
    "}\n",
    "responses2"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Assistant(content=I’m sorry that one didn’t hit the mark—let me try again:\n",
       "\n",
       "Why did the Kotlin program go to therapy?\n",
       "\n",
       "Because it had too many *null* feelings and kept throwing *Exceptions* when it tried to “unpack” its emotions!, metaInfo=ResponseMetaInfo(timestamp=2025-10-16T09:45:52.033873Z, totalTokensCount=305, inputTokensCount=204, outputTokensCount=101, additionalInfo={}, metadata=null), attachments=[], finishReason=null)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T09:46:14.062714Z",
     "start_time": "2025-10-16T09:46:14.012497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ai.koog.prompt.params.LLMParams\n",
    "\n",
    "val chat3 = prompt(chat2) {\n",
    "    messages(responses2)\n",
    "    user {\n",
    "        text(\"Write a Kotlin function that produces some string.\")\n",
    "    }\n",
    "}"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<objective>\n",
       "  Helpfull assistant!\n",
       "</objective>\n",
       "<instructions>\n",
       "  <critical>\n",
       "    Never hallucinate any data, and strictly answer to users questions.\n",
       "    If you cannot answer the question you must say so.\n",
       "  </critical>\n",
       "</instructions>, <user-question>\n",
       "  Tell me a Kotlin Joke.\n",
       "</user-question>, Sure, here’s a quick Kotlin joke for you:\n",
       "\n",
       "Why do Kotlin developers never play hide and seek?\n",
       "\n",
       "Because they always *let* the compiler find them!, <user-question>\n",
       "  That was a bad joke! Tell me another better one.\n",
       "</user-question>, I’m sorry that one didn’t hit the mark—let me try again:\n",
       "\n",
       "Why did the Kotlin program go to therapy?\n",
       "\n",
       "Because it had too many *null* feelings and kept throwing *Exceptions* when it tried to “unpack” its emotions!, Write a Kotlin function that produces some string."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "name": "kotlin",
   "version": "2.2.20-Beta2",
   "mimetype": "text/x-kotlin",
   "file_extension": ".kt",
   "pygments_lexer": "kotlin",
   "codemirror_mode": "text/x-kotlin",
   "nbconvert_exporter": ""
  },
  "ktnbPluginMetadata": {
   "projectLibraries": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
